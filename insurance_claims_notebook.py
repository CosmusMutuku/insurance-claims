# -*- coding: utf-8 -*-
"""INSURANCE_CLAIMS_NOTEBOOK.ipynb


Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DKR4VceBdfn_C3_I5k60fI3iOFJdcIx-


![l8g4efr8_car_120x90_14_March_22.png](attachment:l8g4efr8_car_120x90_14_March_22.png)

### Project Overview: Insurance Claims

The aim of this project is to predict the total claim amount per exposure, for specific risk profiles.    
A risk profile describes a policyholder (someone who is a client and owns a policy at the insurer), things like age, gender, marital status, etc. These attributes can all be found in the dataset used in this project. The reason for doing this is so that the predicted claim can be used to calculate a premium for new applicants, based on their predicted risk. This way the insurer maximises profits by charging higher premiums for higher risk profiles. It is important to consider however that accompanying a higher premium, the insurer should maintain sensibility in deciding on a premium and still outcompete competitor insurers, which make business rules essential.

<a id="cont"></a>

## Table of Contents

<a href=#one>1. Importing Packages</a>

<a href=#two>2. Loading Data</a>

<a href=#three>3. Data Cleaning </a>

<a href=#three>4. Exploratory Data Analysis (EDA)</a>

<a href=#four>5. Data Engineering</a>

<a href=#five>6. Modeling</a>

<a href=#six>7. Model Performance</a>

<a href=#seven>8. Model Explanations</a>

<a id="one"></a>
## 1. Importing Packages
<a href=#cont>Back to Table of Contents</a>
"""

# Commented out IPython magic to ensure Python compatibility.
# Libraries for data loading, data manipulation and data visulisation
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
# %matplotlib inline
from datetime import datetime, date
import warnings

import plotly.express as px
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Binarizer
from sklearn.preprocessing import KBinsDiscretizer
from fast_ml.model_development import train_valid_test_split
from fast_ml.feature_engineering import FeatureEngineering_Categorical
from fast_ml.feature_engineering import FeatureEngineering_Numerical
from fast_ml.feature_engineering import FeatureEngineering_DateTime
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from statsmodels.compat import lzip
from statsmodels.graphics.api import abline_plot
from patsy import dmatrices
import statsmodels.api as sm
from sklearn.utils.validation import check_array
from sklearn import metrics
from sklearn.metrics import mean_squared_error as MSE
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
import math

warnings.filterwarnings("ignore")

"""<a id="two"></a>
## 2. Loading the Data
<a class="anchor" id="1.1"></a>
<a href=#cont>Back to Table of Contents</a>

"""

ins_claim_df = pd.read_excel('DataSet.xlsx')

ins_claim_df.head(5)

# number of observations and features
ins_claim_df.shape

#checking for duplicates
# Rows containing duplicate data
duplicate_rows_df = ins_claim_df[ins_claim_df.duplicated()]
print("number of duplicate rows:", duplicate_rows_df.shape)

duplicate_rows_df.head()

"""These are not actually duplicates, so I will leave it as it is."""

#checking and counting hull values
ins_claim_df.isnull().sum()

### Descriptive analysis
ins_claim_df.describe()

# Datatypes of the dataset
ins_claim_df.dtypes

ins_claim_df.info()

# check for and count duplicates
ins_claim_df.duplicated().sum()

"""<a id="three"></a>
## 2. Data Cleaning
<a class="anchor" id="1.1"></a>
<a href=#cont>Back to Table of Contents</a>  

In this phase of the project, I will be:
- Rename `SumTotalClaimAmount` to `ClaimAmount`.
- Converting some of the text features such as `occupation`, `model`, etc to lower case.
- Drop the last column (`0.5865768182345752`).

"""

ins_claim_df.drop(0.5865768182345752, axis=1, inplace=True)

ins_claim_df.rename(columns={'SumTotalClaimAmount': 'TotalClaimAmount',
                             'CountTotalClaimAmount': 'CountClaim'}, inplace=True)

"""<a id="three"></a>
## 3. Exploratory Data Analysis (EDA)
<a class="anchor" id="1.1"></a>
<a href=#cont>Back to Table of Contents</a>

As observed earlier in  the `ins_claim_df.head(5)`, `ins_claim_df.info()` and `ins_claim_df.isnull().sum()`, some of the values in our target variables, `claimid` and `counttotalclaimamount` are `Null` values. These are not actually null because the policyholder/insured didnt not make claim. And if there is no claim, there will be no `claimid` or `sumtotalclaimamount`.

![200.gif](attachment:200.gif)

### A summary statistics on the features
"""

### Descriptive analysis\
ins_claim_df.describe()

"""The dataframe above tells a brief statistical summary of all the numerical features. Like here I see that the highest count of total claim amount within a period of insurance coverage is 9."""

ins_claim_df.hist(figsize=(20,20));

"""I observed that most of these numerical features are skewed to the right.

### How many claims were made in the dataset?

Let me view the dataframe without the null values in `sumtotalclaimamount`.
"""

# Create a datframe where there are no null values in the `sumtotalclaimamount`
ins_claim = ins_claim_df.loc[ins_claim_df['TotalClaimAmount'].notnull()]
ins_claim.head()

# Create a datframe where there are only null values in the `sumtotalclaimamount`
null_ins_claim = ins_claim_df.loc[ins_claim_df['TotalClaimAmount'].isnull()]
null_ins_claim.head()

# observing the proportion of the non-nul dataframe to the whole data
prop = (ins_claim.shape[0]/ins_claim_df.shape[0])*100
print(f'Only {round(prop, 2)}% of the insurance policies in the whole dataset were claimed')

#visualizing claim occurence
ax = ins_claim_df['TotalClaimAmount'].fillna(0).apply(
               lambda x: ('No' if x == 0 else 'Yes')).value_counts().plot(kind='bar')
ax.yaxis.set_major_formatter(mtick.PercentFormatter(ins_claim_df.shape[0]))

"""What I can conclude here is that most policyholders only make the insurace contract for the sole purpose of protection against unforseen accident or peril. They wouldnt want to start looking for a way to get their money back from the insurance company before policy agreement expires."""



"""### Distribution of our target variable

Let me plot a graph to show the distribution of our target variable which is the `TotalClaimAmount`. There will be two graphs; one will display the TotalClaimAmount with the no(null)claims included. And the other will the display the TotalClaimAmount excluding the no(null)claims.
"""

#create normal distribution curve
fig, axes = plt.subplots(1, 2, figsize=(15, 6))
sns.distplot(ins_claim_df['TotalClaimAmount'].fillna(0), hist=True, kde=True,
             color = 'darkblue', bins=25,
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 2}, ax=axes[0]).set(title='Distribution of TotalClaimAmount(excluding the zero claims)')

sns.distplot(ins_claim['TotalClaimAmount'], hist=True, kde=True,
             color = 'darkblue', bins=25,
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 2}, ax=axes[1]).set(title='Distribution of TotalClaimAmount(excluding the zero claims)');

"""We can notice here that the distribution of our target variable is skewed to the right.

### Insured/Policyholders who made claims and the number of times they made claim within their policy duration.
"""

# visualizing the claim count
# creating a function to plot a bar chart
def most_claim_count(df,column):
    #remove all the zero values
    df = df[df.CountClaim != 0.0]
    #plotting graph
    plt.figure(figsize=(14,7))
    data = df[str(column)].value_counts()
    ax = sns.barplot(x = data.index, y = data, order= data.index, palette='dark', edgecolor="black")
    for p in ax.patches:
        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()),
                fontsize=11, ha='center', va='bottom')
    plt.title('Claim Frequency Distribution', fontsize=14)
    plt.xlabel('Number of claims')
    plt.ylabel('frequency')
    plt.xticks(rotation=90)
    plt.show()

# Call Function
most_claim_count(ins_claim,'CountClaim')

"""### Checking for correlations among the features"""

# evaluate correlation
#plotting a heat map to check the level of correlation between the numerical features
heatmap = sns.heatmap(ins_claim_df.corr(), vmin=-1, vmax=1, annot=True)
sns.set(rc = {'figure.figsize':(20, 15)})
heatmap.set_title('Correlation Heatmap for the Numerical features', fontdict={'fontsize':22}, pad=20);

"""There seem to be no correlation between our target variable and(even among) these numerical features in the dataset except for `premiumsum` and `casepremiumsum`. And little corellation between `meanexcess`, `totalexcess` and `excesstypecount`.

### The gender Frequency   

Let us observe the gender that makes claims the most.
"""

# creating lists for gender and counts
gen_freq = list(ins_claim['Gender'].value_counts())
gen = list(ins_claim['Gender'].unique())
# Plotting the pie chart to count the percentage of
# each gender of the dataset on the above dataframe

explode = (0.03, 0.03)

# Creating autocpt arguments
def func(pct, allvalues):
    absolute = int(pct / 100.*np.sum(allvalues))
    return "{:.1f}%".format(pct, absolute)

# Creating plot
fig, ax = plt.subplots(figsize =(10, 7))
wedges, texts, autotexts = ax.pie(gen_freq,
                                  autopct = lambda pct: func(pct, gen_freq),
                                  explode = explode,
                                  labels = gen,
                                  startangle = 10,
                                  textprops = dict(color ="black"))
# Adding legend
ax.legend(wedges, gen,
          title ="Gender",
          loc ="right",
          bbox_to_anchor =(1, 0, 0.5, 1))

plt.setp(autotexts, size = 20, weight ="light")
ax.set_title("Percentage count of Gender", fontdict={'fontsize':22})
# show plot
plt.show()

"""As seen above, the Males made more claims than females. It is however clear that the males are more exposed to risk when it comes to driving.

### Occupation of policy holders that makes the most claims

There are occupations that involves frequent driving on the road or highway. These vehicles are more exposed to risk or accident.
"""

#converting the occupation column to lowercase
ins_claim['Occupation'] = ins_claim['Occupation'].str.lower()
#replacing special charatcer'-' with space
ins_claim['Occupation'] = ins_claim['Occupation'].replace(to_replace ='-', value = ' ', regex = True)

# drop claims without occupation and plotting a bar chart
ins_claim = ins_claim[ins_claim.Occupation != 'nan']
fig, ax = plt.subplots(figsize=(14, 7))
sns.countplot(x='Occupation', data=ins_claim,
              order=ins_claim['Occupation'].value_counts(ascending=False).head(20).index)
plt.xticks(rotation=90)
plt.xlabel('Occupation', size=20)
plt.ylabel('Claim Count', size=20)
plt.title('Occupation of Policy holders', size=25)
plt.show()

"""Although I expect to see 'driver' as the general occupation of policyholders that make most claims, Educators and teachers could be understandable(maybe driving to and from Institutions everyday).

### Occupation of zero-claims policy holders

I can get this by producing a subset of the `insurance_claim_df` where there are only null values in the `sumtotalclaimamount`.
"""

#converting the occupation column to lowercase
null_ins_claim['Occupation'] = null_ins_claim['Occupation'].str.lower()
#replacing special charatcer'-' with space
null_ins_claim['Occupation'] = null_ins_claim['Occupation'].replace(to_replace ='-', value = ' ', regex = True)

#view as dataframe
null_count = pd.DataFrame(list(null_ins_claim['Occupation'].value_counts()),
                          null_ins_claim['Occupation'].dropna().unique(),
                          columns=['no_claim_count'])

null_count.head()

"""Let me view it in a bar chart"""

null_count.sort_values(by=['no_claim_count'], ascending=False).head(20).plot(
    use_index=True, y='no_claim_count', kind="bar", color= 'red', xlabel='Occupation',
    ylabel='Frequency', title='Most common occupation of No claim Policy holders', figsize=(15, 7));

"""### Gender feature as a factor in Insurance Claim."""

no_claim_m = null_ins_claim['Gender'].value_counts()['Male']
no_claim_f = null_ins_claim['Gender'].value_counts()['Female']
claim_m = ins_claim['Gender'].value_counts()['Male']
claim_f = ins_claim['Gender'].value_counts()['Female']
tot_m = ins_claim_df['Gender'].value_counts()['Male']
tot_f = ins_claim_df['Gender'].value_counts()['Female']
perc_claim_m = ins_claim['Gender'].value_counts()['Male'] / ins_claim_df['Gender'].value_counts()['Male'] * 100
perc_claim_f = ins_claim['Gender'].value_counts()['Female'] / ins_claim_df['Gender'].value_counts()['Female'] * 100
perc_noclaim_m = null_ins_claim['Gender'].value_counts()['Male'] / ins_claim_df['Gender'].value_counts()['Male'] * 100
perc_noclaim_f = null_ins_claim['Gender'].value_counts()['Female'] / ins_claim_df['Gender'].value_counts()['Female'] * 100

gender_data = {'Gender': ['Male', 'Female'],
            'Claims':[claim_m, claim_f],
            'No Claims':[no_claim_m, no_claim_f],
            'Total':[tot_m, tot_f],
              'Claims %':[round(perc_claim_m, 2), round(perc_claim_f, 2)],
              'No Claims %':[round(perc_noclaim_m, 2), round(perc_noclaim_f, 2)]}
gender_data = pd.DataFrame(gender_data)
gender_data

"""From the dataframe above, it can be seen that gender really isnt a factor in insurance claim. Both Male and Female can have a high exposure to risk.

### Checking the age of the policyholders who made claims

Here, I will create a column that calculates the age of the policyholder who made claims.
"""

# This function converts given date to age
def age(born):
    born = datetime.strptime(born, "%Y-%m-%d").date()
    today = date.today()
    return today.year - born.year - ((today.month,
                                      today.day) < (born.month,
                                                    born.day))

# creating age column from the function
ins_claim['Age'] = ins_claim['BirthDt'].astype('str').apply(age)

ins_claim.head()



"""### Vehicle body type versus claims made

Let me observe the vehicle bodytype that frequently makes claims.
"""

#
fig, ax = plt.subplots(figsize=(14, 7))
sns.countplot(x='BodyType', data=ins_claim,
              order=ins_claim['BodyType'].value_counts(ascending=False).index)
plt.xticks(rotation=90)
plt.xlabel('Vehicle Body Types', size=20)
plt.ylabel('Claim count', size=20)
plt.title('claims count by Vehicle Bodytype', size=25)
plt.show()

"""Looking at the most frequent 6 vehicle bodytype, we have:     
H/B -- Hatchback     
S/D -- Sedan     
SUV -- Sport Utility Vehicle     
S/C -- Saloon Car     
D/C -- Design Cars     
MPV -- Multi-Purpose Vehicle     
Majority of the vehicle bodytype are HatchBack. Therefore, vehicle bodytype is a factor to be considered in risk exposure.
"""

#
fig, ax = plt.subplots(figsize=(14, 7))
sns.countplot(x='BodyType', data=null_ins_claim,
              order=null_ins_claim['BodyType'].value_counts(ascending=False).index)
plt.xticks(rotation=90)
plt.xlabel('Vehicle Body Types', size=20)
plt.ylabel('Claim count', size=20)
plt.title('claims count by Vehicle Bodytype', size=25)
plt.show()

"""Relationship between the Insured Vehicle bodytype and the claims."""



#adding temporary columns for counting PD
ins_claim['ClaimAmountRange'] = ins_claim.TotalClaimAmount.apply(
               lambda x: ('R1 - R10k' if x < 11000  else ('R11k - R50k' if x < 50999.99 else (
               'R51k - R100k' if x < 100999.99 else (
               'R101k - R500k' if x < 500999.99 else (
               'R501k - R1m' if x < 1000999.99 else 'R1m and Above'))))))

ins_claim['ClaimAmountRange'].value_counts()

def top_rating_plot(df,column, n):
    #plotting graph
    plt.figure(figsize=(14,7))
    data = df[str(column)].value_counts().head(n)
    ax = sns.barplot(x = data.index, y = data, order= data.index, color='blue')
    for p in ax.patches:
        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()),
                fontsize=15, ha='center', va='bottom')
    plt.title('TotalClaimAmount Frequency', fontsize=12)
    plt.xlabel('TotalClaimAmount', fontsize=15)
    plt.ylabel('Claim Count')
    plt.xticks(rotation=0, size=15 )
    plt.show()

top_rating_plot(ins_claim,'ClaimAmountRange',10)

# creating lists for gender and counts
gen_freq = list(ins_claim['PersonProvince'].value_counts())
gen = list(ins_claim['PersonProvince'].unique())
# Plotting the pie chart to count the percentage of
# each gender of the dataset on the above dataframe

explode = (0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03)

# Creating autocpt arguments
def func(pct, allvalues):
    absolute = int(pct / 100.*np.sum(allvalues))
    return "{:.1f}%".format(pct, absolute)

# Creating plot
fig, ax = plt.subplots(figsize =(10, 7))
wedges, texts, autotexts = ax.pie(gen_freq,
                                  autopct = lambda pct: func(pct, gen_freq),
                                  labels = gen,
                                  explode = explode,
                                  startangle = 10,
                                  textprops = dict(color ="black"))
# Adding legend
ax.legend(wedges, gen,
          title ="Gender",
          loc ="right",
          bbox_to_anchor =(1, 0, 0.5, 1))

plt.setp(autotexts, size = 15, weight ="light")
ax.set_title("Percentage count of Province", fontdict={'fontsize':22})
# show plot
plt.show()

no_claim_kwa = null_ins_claim['PersonProvince'].value_counts()['KWAZULU-NATAL']
no_claim_mpu = null_ins_claim['PersonProvince'].value_counts()['MPUMALANGA']
no_claim_wes = null_ins_claim['PersonProvince'].value_counts()['WESTERN CAPE']
no_claim_gau = null_ins_claim['PersonProvince'].value_counts()['GAUTENG']
no_claim_fre = null_ins_claim['PersonProvince'].value_counts()['FREE STATE']
no_claim_nor = null_ins_claim['PersonProvince'].value_counts()['NORTHERN CAPE']
no_claim_lim = null_ins_claim['PersonProvince'].value_counts()['LIMPOPO']
no_claim_eas = null_ins_claim['PersonProvince'].value_counts()['EASTERN CAPE']
no_claim_nowe = null_ins_claim['PersonProvince'].value_counts()['NORTH WEST']

claim_kwa = ins_claim['PersonProvince'].value_counts()['KWAZULU-NATAL']
claim_mpu = ins_claim['PersonProvince'].value_counts()['MPUMALANGA']
claim_wes = ins_claim['PersonProvince'].value_counts()['WESTERN CAPE']
claim_gau = ins_claim['PersonProvince'].value_counts()['GAUTENG']
claim_fre = ins_claim['PersonProvince'].value_counts()['FREE STATE']
claim_nor = ins_claim['PersonProvince'].value_counts()['NORTHERN CAPE']
claim_lim = ins_claim['PersonProvince'].value_counts()['LIMPOPO']
claim_eas = ins_claim['PersonProvince'].value_counts()['EASTERN CAPE']
claim_nowe = ins_claim['PersonProvince'].value_counts()['NORTH WEST']

tot_kwa = ins_claim_df['PersonProvince'].value_counts()['KWAZULU-NATAL']
tot_mpu = ins_claim_df['PersonProvince'].value_counts()['MPUMALANGA']
tot_wes = ins_claim_df['PersonProvince'].value_counts()['WESTERN CAPE']
tot_gau = ins_claim_df['PersonProvince'].value_counts()['GAUTENG']
tot_fre = ins_claim_df['PersonProvince'].value_counts()['FREE STATE']
tot_nor = ins_claim_df['PersonProvince'].value_counts()['NORTHERN CAPE']
tot_lim = ins_claim_df['PersonProvince'].value_counts()['LIMPOPO']
tot_eas = ins_claim_df['PersonProvince'].value_counts()['EASTERN CAPE']
tot_nowe = ins_claim_df['PersonProvince'].value_counts()['NORTH WEST']

perc_claim_kwa = ins_claim['PersonProvince'].value_counts()['KWAZULU-NATAL'] / ins_claim_df['PersonProvince'].value_counts()['KWAZULU-NATAL'] * 100
perc_claim_mpu = ins_claim['PersonProvince'].value_counts()['MPUMALANGA'] / ins_claim_df['PersonProvince'].value_counts()['MPUMALANGA'] * 100
perc_claim_wes = ins_claim['PersonProvince'].value_counts()['WESTERN CAPE'] / ins_claim_df['PersonProvince'].value_counts()['WESTERN CAPE'] * 100
perc_claim_gau = ins_claim['PersonProvince'].value_counts()['GAUTENG'] / ins_claim_df['PersonProvince'].value_counts()['GAUTENG'] * 100
perc_claim_fre = ins_claim['PersonProvince'].value_counts()['FREE STATE'] / ins_claim_df['PersonProvince'].value_counts()['FREE STATE'] * 100
perc_claim_nor = ins_claim['PersonProvince'].value_counts()['NORTHERN CAPE'] / ins_claim_df['PersonProvince'].value_counts()['NORTHERN CAPE'] * 100
perc_claim_lim = ins_claim['PersonProvince'].value_counts()['LIMPOPO'] / ins_claim_df['PersonProvince'].value_counts()['LIMPOPO'] * 100
perc_claim_eas = ins_claim['PersonProvince'].value_counts()['EASTERN CAPE'] / ins_claim_df['PersonProvince'].value_counts()['EASTERN CAPE'] * 100
perc_claim_nowe = ins_claim['PersonProvince'].value_counts()['NORTH WEST'] / ins_claim_df['PersonProvince'].value_counts()['NORTH WEST'] * 100

perc_noclaim_kwa = null_ins_claim['PersonProvince'].value_counts()['KWAZULU-NATAL'] / ins_claim_df['PersonProvince'].value_counts()['KWAZULU-NATAL'] * 100
perc_noclaim_mpu = null_ins_claim['PersonProvince'].value_counts()['MPUMALANGA'] / ins_claim_df['PersonProvince'].value_counts()['MPUMALANGA'] * 100
perc_noclaim_wes = null_ins_claim['PersonProvince'].value_counts()['WESTERN CAPE'] / ins_claim_df['PersonProvince'].value_counts()['WESTERN CAPE'] * 100
perc_noclaim_gau = null_ins_claim['PersonProvince'].value_counts()['GAUTENG'] / ins_claim_df['PersonProvince'].value_counts()['GAUTENG'] * 100
perc_noclaim_fre = null_ins_claim['PersonProvince'].value_counts()['FREE STATE'] / ins_claim_df['PersonProvince'].value_counts()['FREE STATE'] * 100
perc_noclaim_nor = null_ins_claim['PersonProvince'].value_counts()['NORTHERN CAPE'] / ins_claim_df['PersonProvince'].value_counts()['NORTHERN CAPE'] * 100
perc_noclaim_lim = null_ins_claim['PersonProvince'].value_counts()['LIMPOPO'] / ins_claim_df['PersonProvince'].value_counts()['LIMPOPO'] * 100
perc_noclaim_eas = null_ins_claim['PersonProvince'].value_counts()['EASTERN CAPE'] / ins_claim_df['PersonProvince'].value_counts()['EASTERN CAPE'] * 100
perc_noclaim_nowe = null_ins_claim['PersonProvince'].value_counts()['NORTH WEST'] / ins_claim_df['PersonProvince'].value_counts()['NORTH WEST'] * 100

location_data = {'Province': ['KWAZULU-NATAL', 'MPUMALANGA', 'WESTERN CAPE', 'GAUTENG',
                          'FREE STATE', 'NORTHERN CAPE', 'LIMPOPO', 'EASTERN CAPE',
                          'NORTH WEST'],
                 'Claims':[claim_kwa, claim_mpu, claim_wes, claim_gau, claim_fre,
                      claim_nor, claim_lim, claim_eas, claim_nowe],
                 'No Claims':[no_claim_kwa, no_claim_mpu, no_claim_wes, no_claim_gau,
                         no_claim_fre, no_claim_nor, no_claim_lim, no_claim_eas, no_claim_nowe],
                 'Total':[tot_kwa, tot_mpu, tot_wes, tot_gau,
                     tot_fre, tot_nor, tot_lim, tot_eas, tot_nowe],
                 'Claims %':[round(perc_claim_kwa, 2), round(perc_claim_mpu, 2),
                          round(perc_claim_wes, 2), round(perc_claim_gau, 2),
                          round(perc_claim_fre, 2), round(perc_claim_nor, 2),
                          round(perc_claim_lim, 2), round(perc_claim_eas, 2), round(perc_claim_nowe, 2)],
                 'No Claims %':[round(perc_noclaim_kwa, 2), round(perc_noclaim_mpu, 2),
                          round(perc_noclaim_wes, 2), round(perc_noclaim_gau, 2),
                          round(perc_noclaim_fre, 2), round(perc_noclaim_nor, 2),
                          round(perc_noclaim_lim, 2), round(perc_noclaim_eas, 2), round(perc_noclaim_nowe, 2)]}
location_data = pd.DataFrame(location_data)
location_data



"""### Age Category"""

#adding temporary columns for counting PD
ins_claim['AgeRange'] = ins_claim.Age.apply(
               lambda x: ('16 - 25years' if x <= 25 else ('26 - 35years' if x <= 35 else (
               '36 - 45years' if x <= 45 else (
               '46 - 55years' if x <= 55 else (
               '56 - 65years' if x <= 65 else '66years and Above'))))))

ins_claim.head()

def top_rating_plot(df,column, n):
    #plotting graph
    plt.figure(figsize=(14,7))
    data = df[str(column)].value_counts().head(n)
    ax = sns.barplot(x = data.index, y = data, order= data.index, color='brown')
    for p in ax.patches:
        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()),
                fontsize=15, ha='center', va='bottom')
    plt.title('AgeRange claim Frequency', fontsize=22)
    plt.xlabel('TotalClaimAmount')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()

top_rating_plot(ins_claim,'AgeRange',10)

# creating lists for gender and counts
gen_freq = list(ins_claim['AgeRange'].value_counts())
gen = list(ins_claim['AgeRange'].unique())
# Plotting the pie chart to count the percentage of
# each gender of the dataset on the above dataframe

explode = (0.03, 0.03, 0.03, 0.03, 0.03, 0.03)

# Creating autocpt arguments
def func(pct, allvalues):
    absolute = int(pct / 100.*np.sum(allvalues))
    return "{:.1f}%".format(pct, absolute)

# Creating plot
fig, ax = plt.subplots(figsize =(13, 9))
wedges, texts, autotexts = ax.pie(gen_freq,
                                  autopct = lambda pct: func(pct, gen_freq),
                                  labels = gen,
                                  explode = explode,
                                  startangle = 10,
                                  textprops = dict(color ="black"))
# Adding legend
ax.legend(wedges, gen,
          title ="Age Range",
          loc ="right",
          bbox_to_anchor =(1, 0, 0.5, 1))

plt.setp(autotexts, size = 15, weight ="light")
ax.set_title("Proportion of Age Category in claims made", fontdict={'fontsize':22})
# show plot
plt.show()

# function to subtract curent year from vehicle Age
def vehicleAge(born):
    return 2022 - born

# creating age column from the function
null_ins_claim['VehicleAge'] = null_ins_claim['VehicleYear'].apply(vehicleAge)
ins_claim['VehicleAge'] = ins_claim['VehicleYear'].apply(vehicleAge).astype(int)

ins_claim.head()

# visualizing the age of insured vehicles by claims made
with sns.axes_style('darkgrid'):
    g = sns.catplot("VehicleAge", data=ins_claim, aspect=2.5, kind='count')
    g.set_ylabels(" Total number of claims made");

fig, (ax1, ax2) = plt.subplots(ncols=2)
fig.set_size_inches(19,6)
g = sns.countplot(x='VehicleType',data=ins_claim.sort_values('VehicleType'), ax=ax1)
title = g.set_title('Vehicle type count on claims made')
g = sns.countplot(x='PolicyStatus',data=ins_claim.sort_values('PolicyStatus'), ax=ax2)
title = g.set_title('Vehicle type count on claims made')

fig, axes = plt.subplots(2, 2, figsize=(18, 10))
fig.suptitle('Categories by Total Claim Amount')
sns.barplot(ax=axes[0, 0], data=ins_claim, x='EmploymentType', y='TotalClaimAmount')
sns.barplot(ax=axes[0, 1], data=ins_claim, x='IndustryType', y='TotalClaimAmount')
sns.barplot(ax=axes[1, 0], data=ins_claim, x='MaritalStatus', y='TotalClaimAmount')
sns.barplot(ax=axes[1, 1], data=ins_claim, x='Colour', y='TotalClaimAmount');

fig, axes = plt.subplots(2, 2, figsize=(18, 10))
fig.suptitle('Categories by Total Claim Amount')
sns.barplot(ax=axes[0, 0], data=ins_claim, x='Gender', y='TotalClaimAmount')
sns.barplot(ax=axes[0, 1], data=ins_claim, x='VehicleType', y='TotalClaimAmount')
sns.barplot(ax=axes[1, 0], data=ins_claim, x='VehicleAge', y='TotalClaimAmount')
sns.barplot(ax=axes[1, 1], data=ins_claim, x='PersonProvince', y='TotalClaimAmount');

fig, axes = plt.subplots(2, 2, figsize=(18, 10))
fig.suptitle('Categories by Total Claim Amount')
sns.barplot(ax=axes[0, 0], data=ins_claim, x='PolicyStatus', y='TotalClaimAmount')
sns.barplot(ax=axes[0, 1], data=ins_claim, x='VehicleType', y='TotalClaimAmount')
sns.barplot(ax=axes[1, 0], data=ins_claim, x='BodyType', y='TotalClaimAmount')
sns.barplot(ax=axes[1, 1], data=ins_claim, x='PersonProvince', y='TotalClaimAmount');

"""<a id="four"></a>
## 4. Data Engineering
<a class="anchor" id="1.1"></a>
<a href=#cont>Back to Table of Contents</a>

Next, on this phase of the project, I am going to:
- Replace `null` values with zero(0) on the `TotaClaimAmount` feature.
- Replace `null` values with the mode on some of the categorical features with very few null values such as `PolicyStatus`, `Gender`, `Transmission`, `BodyType`, `Colour` and more.
- Create the Target variable `AverageClaimAmount` which will be gotten from `TotalClaimAmount` and `CountClaim`.
- Drop the columns that will no be useful to training the model(s) like `Claimid`, `Policyid`, `PremiumSum`, `MonthsSinceInception` and more.
- Create other features like `VehicleAge`, `ClaimFrequency`, `Exposure`, `Age`.
- Clean messy features like `occupation` and `Model`.

I will be performing these actions on the complete dataset.

#### Replacing missing value with 0 in `Totalclaimamount` and `Countclaim` columns  (*Done by Chukwudi*)
"""

# replace missing value with 0
ins_claim_df['TotalClaimAmount'] = ins_claim_df['TotalClaimAmount'].fillna(0)
ins_claim_df['CountClaim'] = ins_claim_df['CountClaim'].fillna(0)
ins_claim_df.head()

ins_claim_df.isnull().sum()

"""#### Replacing missing values in categorical features with their mode (*Done by Chukwudi*)"""

# Selecting the neccessary features with few null values into a list
columns = ['EmploymentType', 'Occupation', 'IndustryType', 'Gender' , 'BirthDt', 'MaritalStatus',
           'Model', 'Colour', 'Transmission', 'VehicleType' , 'BodyType', 'Cyl', 'Kilowatts',
           'VehicleYear', 'PolicyMainDriverAnnualMileage', 'PolicyMainDriverLicenseDurationRange',
           'PersonProvince', 'PolicyStatus', 'NomimatedDriversCount', 'CubicCapacity', 'ExcessTypesCount']

#checking their mode values
mode_values = ins_claim_df.filter(columns).mode()
mode_values

# filling them with their mode value
ins_claim_df[columns] = ins_claim_df[columns].fillna(ins_claim_df.mode().iloc[0])

#check the dataframe
ins_claim_df.isnull().sum()

"""#### Replacing missing values in numerical features with their mean (*Done by Chukwudi*)"""

#selecting specific features to be filled
num_columns = ["TotalExcess", "MeanExcess", "SumAssured", "BaseExcess",
               "PreviousInsurerExcess", "PreviousInsurerPremium"]

#checking their mean values
mean_values = ins_claim_df.filter(num_columns).mean()
mean_values

# replace missing value with mean
for m in num_columns:
    ins_claim_df[m].fillna(ins_claim_df[m].mean(), inplace=True)

ins_claim_df.head()

#check the dataframe
ins_claim_df.isnull().sum()

"""#### Creating the target variable `AverageClaimAmount`         (*Done by Elizabeth*)
This can be gotten by dividing the `TotalClaimAmount` by `Countclaim`
"""

# creating column using lambda
ins_claim_df['AverageClaimAmount'] = ins_claim_df.apply(lambda x: x['TotalClaimAmount'] if x['TotalClaimAmount'] < 1 else
                                                        x['TotalClaimAmount']/x['CountClaim'], axis=1)

#check the dataframe
ins_claim_df.tail()

"""#### Cleaning the `Occupation` column.     (*Done by Chukwudi*)

The `Occupation` features contain inconsistent string values. For example, to Pandas, `Self Employed`, `selfemployed` and `self-employed` are three different unique values. So I will convert all values to lowercase and remove special characters if any. The categorical values are also mtoo many, so I will select the top 20 values and convert the remaining vales to `others`.
"""

#This was done on the EDA but not to the full dataset.
#converting the occupation column to lowercase
ins_claim_df['Occupation'] = ins_claim_df['Occupation'].str.lower()
#replacing special charatcer'-' with space
ins_claim_df['Occupation'] = ins_claim_df['Occupation'].replace(to_replace ='-', value = ' ', regex = True)

ins_claim_df['Occupation'].value_counts().head(20)

#replacing occupations with their general name
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*nurse.*$)', 'nurse')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*consult.*$)', 'consultant')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*admin.*$)', 'administrator')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*lecture.*$)', 'teacher')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*artisan.*$)', 'artisan')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*police.*$)', 'police officer')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*supervisor.*$)', 'supervisor')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*manager.*$)', 'manager')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*driver.*$)', 'driver')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*self.*$)', 'self employed')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*worker.*$)', 'worker')
ins_claim_df['Occupation'] = ins_claim_df.Occupation.str.replace(r'(^.*constable.*$)', 'police officer')

# make a list of the most frequent categories of the column
top_20_occurring_occ = [occ for occ in ins_claim_df.Occupation.value_counts().sort_values(
    ascending = False).head(10).index]
top_20_occurring_occ

# list of other occupation
occupations = list(ins_claim_df['Occupation'].unique())
others = [ x for x in occupations if x not in top_20_occurring_occ ]

def group_occupation(input_df, others):
    for title in others:
        input_df['Occupation'].replace(title, 'other', inplace=True)

    return input_df

ins_claim_df = group_occupation(ins_claim_df, others)

ins_claim_df.Occupation.head(10)

ins_claim_df.Occupation.nunique()

"""#### Cleaning the `Model` column       (*Done by Ayotunde*)

The model of the insured vehicle can serve as an important feature for building our model. But we have to group and rename these car models(the values) in the `Model` column by their various model names.
"""

#converting the model column to lowercase
ins_claim_df['Model'] = ins_claim_df['Model'].str.lower()
ins_claim_df['Model'].value_counts().head(20)

# extracting the first word in model values
ins_claim_df['Model'] = ins_claim_df.Model.str.split(" ").str.get(0)
ins_claim_df['Model'].value_counts().head(20)

# renaming the values in the Model column with 'Make Model'
ins_claim_df['Model'] = ins_claim_df['Model'].replace({'polo': 'volkswagen polo', 'corolla': 'toyota corolla',
                                                                'i20': 'hyundai i20', 'etios': 'toyota etios',
                                                                'kwid': 'renault kwid', 'grand': 'hyundai grand',
                                                                'sandero': 'dacia sandero', 'picanto': 'kia picanto',
                                                                'fiesta': 'ford fiesta', 'np200': 'nissan np200',
                                                                'golf': 'volkswagen golf', 'clio': 'renault clio',
                                                                'rio': 'kia rio', 'hilux': 'toyota hilux',
                                                                'figo': 'ford figo', 'ranger': 'ford ranger',
                                                                'yaris': 'toyota yaris', '320i': 'bmw 320i',
                                                                'go': 'datsun go', 'a4': 'audi a4'})

ins_claim_df['Model'].value_counts().head(20)

cars = [model for model in ins_claim_df.Model.value_counts().sort_values(ascending = False).head(10).index]

# list of other cars
all_cars = list(ins_claim_df['Model'].unique())
others = [ x for x in all_cars if x not in cars ]

def group_carmodel(input_df, others):
    for modelname in others:
        input_df['Model'].replace(modelname, 'other', inplace=True)

    return input_df

ins_claim_df = group_carmodel(ins_claim_df, others)

ins_claim_df.Model.head(10)

ins_claim_df.Model.unique()

"""#### Cleaning the `Colour` column.  (*Done by Elizabeth*)"""

#converting the occupation column to lowercase
ins_claim_df['Colour'] = ins_claim_df['Colour'].str.lower()
ins_claim_df['Colour'].value_counts().head(20)

colours = [col for col in ins_claim_df.Colour.value_counts().sort_values(ascending = False).head(10).index]

# list of other cars
all_colours = list(ins_claim_df['Colour'].unique())
others = [ x for x in all_colours if x not in colours]

def group_colour(input_df, others):
    for colourname in others:
        input_df['Colour'].replace(colourname, 'other', inplace=True)

    return input_df

ins_claim_df = group_colour(ins_claim_df, others)

ins_claim_df.Colour.head(10)

ins_claim_df.Colour.unique()

"""#### Cleaning `IndustryType` column     (*Done by Kayode*)"""

# converting to lowercase
ins_claim_df['IndustryType'] = ins_claim_df['IndustryType'].str.lower()
ins_claim_df['IndustryType'].value_counts().head(20)

# renaming values in the industrytype column
ins_claim_df['IndustryType'] = ins_claim_df.IndustryType.str.replace(r'(^.*trade.*$)', 'trade')
ins_claim_df['IndustryType'] = ins_claim_df.IndustryType.str.replace(r'(^.*education.*$)', 'education')
ins_claim_df['IndustryType'] = ins_claim_df.IndustryType.str.replace(r'(^.*office.*$)', 'office')
ins_claim_df['IndustryType'].value_counts().head(20)



"""#### Creating the `Age` column (*Done by Elizabeth*)

Here, I will derive the age of the policyholder from the `BirthDt` column.
"""

# function to convert given date to age
def age(born):
    born = datetime.strptime(born, "%Y-%m-%d").date()
    today = date.today()
    return today.year - born.year - ((today.month,
                                      today.day) < (born.month,
                                                    born.day))

# creating age column from the function
ins_claim_df['Age'] = ins_claim_df['BirthDt'].astype('str').apply(age)

"""#### Creating the `VehicleAge` column (*Done by Chukwudi*)"""

# function to subtract curent year from vehicle Age
def vehicleAge(born):
    return 2022 - born

# creating vehicleage column from the function
ins_claim_df['VehicleAge'] = ins_claim_df['VehicleYear'].apply(vehicleAge)
ins_claim_df['VehicleAge'] = ins_claim_df['VehicleYear'].apply(vehicleAge).astype(int)

"""#### Creating the `Exposure` feature      (*Done by Sam*)

Exposure is a fraction of the year during which the policyholder is exposed to risk.

To get this, I will first make sure that the start-date and the end-date of the insurance policy is in pandas `datetime`. If the policy has not ended(active), I will replace null values with present date.
"""

#creating the Exposure column for the overall dataframe
ins_claim_df['PolicyEndDate'] = pd.to_datetime(ins_claim_df['PolicyEndDate'])
ins_claim_df["Exposure"] = [(-1*round((x-y).total_seconds()/(60*60*24))/365)
                            if pd.isna(y)==False and (-1*round((x-y).total_seconds()/(60*60*24))/365) < 1
                            else 1 for x,y in zip(ins_claim_df["CommencementDt"], ins_claim_df["PolicyEndDate"])]

#calculate the number of days between two days
#ins_claim_df['PolicyEndDate'] = pd.to_datetime(ins_claim_df['PolicyEndDate'])
#ins_claim_df['PolicyEndDate'] = ins_claim_df['PolicyEndDate'].fillna(pd.datetime.now().strftime("%Y-%m-%d"))
#ins_claim_df['DaysSinceInception'] = (ins_claim_df['PolicyEndDate'] - ins_claim_df['CommencementDt']).dt.days

"""#### Creating ClaimFrequency Column     (*Done by Sam*)

The claim frequency rate is a rate which can be estimated as the number of claims divided by the number of units of exposure.
"""

#creating the frequency column for the overall dataframe
ins_claim_df["ClaimFrequency"] = (ins_claim_df["CountClaim"]/ins_claim_df["Exposure"])

ins_claim_df.tail(5)

#visualizing claim occurence

ax = ins_claim_df['Exposure'].plot(kind='hist', figsize=(8,6))
ax.yaxis.set_major_formatter(mtick.PercentFormatter(ins_claim_df.shape[0]))

"""#### Dropping Unnecessary columns"""

#view a list of all the columns on the dataset
ins_claim_df.columns

# List of columns to be dropped
drop_cols = ['PolicyId', 'ClaimId ', 'TotalClaimAmount', 'PremiumSum','CasePremiumSum',
             'PaymentDueDate', 'TotalPremiumPaymentCount', 'IsMemberPayer', 'Employer',
             'BirthDt', 'Make', 'VehicleYear',  'Area', 'MonthsSinceInception', 'CountClaim',
             'DaysSinceInception', 'CommencementDt', 'PolicyEndDate',
             'ClaimFrequency', 'NomimatedDriversUnder30Count']

ins_claim_df.drop(columns=drop_cols, inplace=True)

ins_claim_df.shape

#plotting a heat map to check the level of correlation between the numerical features
numeric_columns = ins_claim_df.dtypes[ins_claim_df.dtypes != "object"].index
heatmap = sns.heatmap(ins_claim_df[numeric_columns].corr(), vmin=-1, vmax=1, annot=True)
sns.set(rc = {'figure.figsize':(15,10)})
heatmap.set_title('Correlation Heatmap for the Numerical features', fontdict={'fontsize':12}, pad=20);

"""#### Bining the `CubicCapacity` column.      
The `cubic capacity` column contains continuous numbers as values.
"""

ins_claim_df['CubicCapacity'].head()

"""Here, we will put these values in bins based on ranges. Let us view this feature on histogram. It will help us determine the range to put the values in.

"""

# Plotting histogram for cubic capacity
#cc = ins_claim_df['CubicCapacity']
#plt.figure(figsize=(6,6))
#plt.hist(cc, bins=5)
#plt.show()

#ins_claim_df['CubicCapacity'][ins_claim_df['CubicCapacity'] > 6000].count()



"""#### Performing one hot encoding on categorical features  (*Done by Elizabeth*)"""

#select categorical datatypes
categorical_features = ins_claim_df.select_dtypes(include=['object']).columns.tolist()

#using dummy encoder for one hot encoding
ins_claim_df_ohe = ins_claim_df

for col in categorical_features:
    col_ohe = pd.get_dummies(ins_claim_df[col], prefix=col)
    ins_claim_df_ohe = pd.concat((ins_claim_df_ohe, col_ohe), axis=1).drop(col, axis=1)

ins_claim_df_ohe.shape



# instantiate sklearn onehote encoder
#ohe = OneHotEncoder()

# transform the categorical features
#transformed_data = ohe.fit_transform(ins_claim_df[categorical_features])

# the above transformed_data is an array so convert it to dataframe
#encoded_data = pd.DataFrame(transformed_data, index=ins_claim_df.index)

# concatenate the original data and the encoded data using pandas
#ins_claim_df_ohe = pd.concat([ins_claim_df, encoded_data], axis=1)



"""#### Performing Binarization on selected features     (*Done by Sam*)

Here, we will scale some of the numerical features to be used for modeling. Scaling involves bring the values in multiple features to the same size or magnitude.
"""

#specifying on the number of bins to produce
n_bins = 10
#selecting specific features to be binarized
binarize_columns = ["ExcessTypesCount", "TotalExcess", "MeanExcess", "SumAssured",
                        "CubicCapacity", "PreviousInsurerExcess", "PreviousInsurerPremium"]
#choosing my prefered binarizer
binarizer = KBinsDiscretizer(n_bins=n_bins, encode='nominal', strategy='uniform')

#ins_claim_df_ohe[binarize_columns] = binarizer.fit_transform(ins_claim_df_ohe[binarize_columns])

"""#### Scaling selected features

Here, we will scale some of the numerical features to be used for modeling. Scaling involves bring the values in multiple features to the same size or magnitude.
"""

numerical_features = ['Cyl', 'Kilowatts', 'BaseExcess',
                      'NomimatedDriversCount', 'Age', 'VehicleAge']

# Initialize StandardScaler from sklearn
scaler = StandardScaler()

# Save standardized features into new variable
#ins_claim_df_ohe[numerical_features] = scaler.fit_transform(ins_claim_df_ohe[numerical_features])

ins_claim_df_ohe.select_dtypes(include=['object']).columns.tolist()

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

"""<a id="five"></a>
## 5. Modelling
<a class="anchor" id="1.1"></a>
<a href=#cont>Back to Table of Contents</a>

---
    
| ⚡ Description: Modelling ⚡ |
| :--------------------------- |
| In this section, you are required to create one or more regression models

---
"""

ins_claim_df_ohe.head(10)

ins_claim_df_ohe.shape

X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(ins_claim_df_ohe,
                                                                            target = 'AverageClaimAmount',
                                                                            train_size=0.7, valid_size=0.2,
                                                                            test_size=0.1)

print(X_train.shape), print(y_train.shape)
print(X_valid.shape), print(y_valid.shape)
print(X_test.shape), print(y_test.shape)

# binarizing (fit and transform) the X_train data
X_train[binarize_columns] = binarizer.fit_transform(X_train[binarize_columns])

# scaling (fit and transform) the X_train data
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])

# binarizing (transform only) the X_valid and X_test data
X_valid[binarize_columns] = binarizer.transform(X_valid[binarize_columns])
X_test[binarize_columns] = binarizer.transform(X_test[binarize_columns])

# scale (transform only) the X_valid and X_test data
X_valid[numerical_features] = scaler.transform(X_valid[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

X_train["Exposure"].isna().sum()

ins_claim_df_ohe.head()

# create one or more ML models
#Initialise the model

lm = LinearRegression()
#Fit the model into training set
lm.fit(X_train, y_train)

# Make predictions on the train sets
y_train_pred = lm.predict(X_train)

# Make predictions on the validation sets
y_val_pred = lm.predict(X_valid)

# Make predictions on the test sets
y_test_pred = lm.predict(X_test)

# Print empirical risk on both sets
print('RMSE on training set:')
print(np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))
print('RMSE on validation set:')
print(np.sqrt(metrics.mean_squared_error(y_valid, y_val_pred)))
print('RMSE on test set:')
print(np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))
print('')

# Print empirical risk on both sets
print('MAE on training set:')
print(metrics.mean_absolute_error(y_train, y_train_pred))
print('MAE on validation set:')
print(metrics.mean_absolute_error(y_valid, y_val_pred))
print('MAE on test set:')
print(metrics.mean_absolute_error(y_test, y_test_pred))
print('')

X_test

#def plot_predicted_vs_actual_distribution(y_test_predict, y_test, feature_name="AverageClaimAmount", logscale = False):
    #fig, ax1 = plt.subplots()
    #sns.kdeplot(x=y_test_predict, ax=ax1, c="Red", label="Predicted", marker=".")
    #plt.legend()
    #ax2 = ax1.twinx()
    #sns.kdeplot(x=y_test[feature_name], ax=ax2, label="Actual", log_scale=logscale)
    #plt.legend()
    #plt.grid()
    #plt.show()

    #return

#def plot_predicted_vs_actual_line(y_test_predict, y_test, feature_name="AverageClaimAmount"):
    #plt.plot(y_test_predict, y_test[feature_name], ".")
    #plt.plot(y_test[feature_name],y_test[feature_name], color = "Blue", linestyle="--")
    #plt.grid()
    #plt.xlabel("Predicted")
    #plt.xlabel("Actual")
    #plt.show()

    #return

fig, ax1 = plt.subplots(figsize=(12,10))
sns.kdeplot(x=y_test_pred, ax=ax1, c="Red", label="Predicted", marker=".")
plt.legend()
ax2 = ax1.twinx()
sns.kdeplot(x=y_test, ax=ax2, label="Actual", log_scale=False)
plt.legend()
plt.grid()
plt.show()

# evaluate one or more ML models

sns.distplot(y_test_pred, color = 'blue', label="Predicted")
#sns.distplot(y_test, color = 'red', label="Actual")

#listed2 =  list(y_test)
#listed3 = list(y_test_pred)
#df = pd.DataFrame(list(zip(listed2, listed3)),
               #columns =['Actual', 'Predicted'])
#df.head(5)
#df.to_csv('pred.csv', index = False)



"""<a id="six"></a>
## 6. Model Performance
<a class="anchor" id="1.1"></a>
<a href=#cont>Back to Table of Contents</a>

---
    
| ⚡ Description: Model performance ⚡ |
| :--------------------------- |
| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |

---
"""

# Compare model performance

# Choose best model and motivate why it is the best choice

"""<a id="seven"></a>
## 7. Model Explanations
<a class="anchor" id="1.1"></a>
<a href=#cont>Back to Table of Contents</a>

---
    
| ⚡ Description: Model explanation ⚡ |
| :--------------------------- |
| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |

---
"""

# discuss chosen methods logic
